# ============================================================
# EU Compliance Scanner v2.0 — Enhanced Pipeline
# ============================================================
# Coverage:
#   DORA (EU) 2022/2554  — ALL 5 Chapters, Articles 5–44
#   NIS2 (EU) 2022/2555  — Articles 21, 23
#   SBOM                 — CycloneDX 1.6 + SPDX 2.3
#
# Multi-Language Support:
#   Python (.py), JavaScript (.js), TypeScript (.ts),
#   Java (.java), Go (.go), Rust (.rs), C/C++ (.c/.cpp/.h),
#   Kotlin (.kt), Scala (.scala), Ruby (.rb), PHP (.php),
#   YAML/JSON configs, Dockerfile, Terraform (.tf),
#   Helm charts, Kubernetes manifests
#
# GPU/AI Code Scanning:
#   CUDA (.cu/.cuh), OpenCL (.cl), SYCL,
#   Python AI frameworks (PyTorch, TensorFlow, JAX),
#   Model files (ONNX, HuggingFace), Jupyter notebooks (.ipynb)
# ============================================================

name: EU Compliance Scanner v2 — DORA | NIS2 | SBOM | GPU/AI

on:
  push:
    branches: [main, develop, release/**]
  pull_request:
    branches: [main, develop]
  schedule:
    - cron: '0 6 * * 1'    # Weekly Monday 06:00 UTC
  workflow_dispatch:
    inputs:
      scan_mode:
        description: 'Scan mode: full | dora-only | nis2-only | gpu-only'
        required: false
        default: 'full'
      fail_on_severity:
        description: 'Fail threshold: CRITICAL | HIGH | MEDIUM'
        required: false
        default: 'CRITICAL'

env:
  REPORT_DIR: compliance-reports
  SBOM_DIR: sbom-output
  FINAL_DIR: final-report
  FAIL_ON_CRITICAL: "true"
  PYTHON_VERSION: "3.11"

# ============================================================
# JOB 1: SBOM Generation (Extended)
# EU AI Act Art.13 + NIS2 Art.21 + DORA Art.8
# ============================================================
jobs:
  sbom-generation:
    name: "SBOM — CycloneDX 1.6 + SPDX 2.3"
    runs-on: ubuntu-latest
    outputs:
      sbom-path: ${{ steps.generate.outputs.sbom_path }}
      component-count: ${{ steps.generate.outputs.component_count }}

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install SBOM & scanning tools
        run: |
          # Syft — SBOM generator
          curl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh | sh -s -- -b /usr/local/bin

          # CycloneDX CLI — format validation
          wget -q https://github.com/CycloneDX/cyclonedx-cli/releases/latest/download/cyclonedx-linux-x64 \
            -O /usr/local/bin/cyclonedx && chmod +x /usr/local/bin/cyclonedx

          # pip-audit — Python-specific deep audit
          pip install pip-audit --quiet

          mkdir -p ${{ env.SBOM_DIR }} ${{ env.REPORT_DIR }}

      - name: Generate multi-format SBOM
        id: generate
        run: |
          # CycloneDX 1.6 — primary (EU AI Act preferred)
          syft . -o cyclonedx-json=${{ env.SBOM_DIR }}/sbom-cyclonedx.json

          # SPDX 2.3 — secondary (interoperability)
          syft . -o spdx-json=${{ env.SBOM_DIR }}/sbom-spdx.json

          # Human-readable table
          syft . -o table=${{ env.SBOM_DIR }}/sbom-table.txt

          COUNT=$(python3 -c "
          import json
          with open('${{ env.SBOM_DIR }}/sbom-cyclonedx.json') as f:
              d = json.load(f)
          print(len(d.get('components', [])))
          ")
          echo "component_count=$COUNT" >> $GITHUB_OUTPUT
          echo "sbom_path=${{ env.SBOM_DIR }}/sbom-cyclonedx.json" >> $GITHUB_OUTPUT
          echo "SBOM generated: $COUNT components"

      - name: Deep Python dependency audit (pip-audit)
        run: |
          # Python-specific vulnerability audit — finds issues Trivy may miss
          if find . -name "requirements*.txt" -o -name "Pipfile.lock" -o -name "pyproject.toml" 2>/dev/null | head -1 | grep -q .; then
            pip-audit --format json --output ${{ env.REPORT_DIR }}/python-audit.json || true
            echo "Python pip-audit completed"
          else
            echo '{"vulnerabilities": []}' > ${{ env.REPORT_DIR }}/python-audit.json
            echo "No Python dependency files found — skipping pip-audit"
          fi

      - name: Validate SBOM completeness
        run: |
          python3 << 'EOF'
          import json, sys, os

          with open("${{ env.SBOM_DIR }}/sbom-cyclonedx.json") as f:
              sbom = json.load(f)

          violations = []
          components = sbom.get("components", [])

          for comp in components:
              name = comp.get("name", "unknown")
              ver  = comp.get("version", "")
              lics = comp.get("licenses", [])
              sup  = comp.get("supplier") or comp.get("author")

              if not lics:
                  violations.append(f"MISSING_LICENSE: {name}@{ver}")
              if not ver or ver.lower() in ["latest", "unknown", ""]:
                  violations.append(f"MISSING_VERSION: {name}")
              if not sup:
                  violations.append(f"MISSING_SUPPLIER: {name}@{ver}")

          os.makedirs("${{ env.REPORT_DIR }}", exist_ok=True)
          with open("${{ env.REPORT_DIR }}/sbom-violations.json", "w") as f:
              json.dump({"violations": violations, "total": len(violations)}, f, indent=2)

          print(f"[SBOM] {len(violations)} violations found across {len(components)} components")
          for v in violations[:20]:
              print(f"  ⚠ {v}")
          if len(violations) > 20:
              print(f"  ... and {len(violations) - 20} more")
          EOF

      - uses: actions/upload-artifact@v4
        with:
          name: sbom-files
          path: |
            ${{ env.SBOM_DIR }}/
            ${{ env.REPORT_DIR }}/sbom-violations.json
            ${{ env.REPORT_DIR }}/python-audit.json
          retention-days: 365  # DORA Art.11 — 1-year minimum retention


  # ============================================================
  # JOB 2: DORA — Full Chapter Coverage
  # ============================================================
  dora-chapter2-risk-management:
    name: "DORA Ch.II — ICT Risk Management (Art.5–16)"
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: DORA Art.5 — Governance & Board Accountability
        run: |
          python3 << 'EOF'
          import os, json

          violations = []

          # Art.5: Management body must actively oversee ICT risk
          governance_docs = [
              "docs/ict-governance.md", "docs/governance.md",
              "docs/risk-governance.md", "GOVERNANCE.md",
              "docs/board-ict-oversight.md"
          ]
          if not any(os.path.exists(p) for p in governance_docs):
              violations.append({
                  "article": "DORA-Art5", "severity": "HIGH",
                  "violation": "No ICT governance or board oversight documentation found",
                  "requirement": "Art.5 requires management body to define and approve ICT risk strategy",
                  "remediation": "Create docs/ict-governance.md documenting board-level ICT risk oversight"
              })

          # Art.5(2): ICT risk as part of overall risk management
          risk_mgmt_docs = [
              "docs/risk-management.md", "docs/ict-risk-framework.md",
              "risk/", "docs/risk/"
          ]
          if not any(os.path.exists(p) for p in risk_mgmt_docs):
              violations.append({
                  "article": "DORA-Art5", "severity": "HIGH",
                  "violation": "No ICT risk management framework documentation",
                  "requirement": "Art.5 requires documented ICT risk management framework",
                  "remediation": "Create docs/ict-risk-framework.md with risk identification, assessment, treatment"
              })

          os.makedirs("${{ env.REPORT_DIR }}", exist_ok=True)
          with open("${{ env.REPORT_DIR }}/dora-art5-governance.json", "w") as f:
              json.dump(violations, f, indent=2)
          print(f"[DORA-Art5] {len(violations)} violations")
          EOF

      - name: DORA Art.8 — ICT Asset Identification & Classification
        run: |
          python3 << 'EOF'
          import os, json, glob

          violations = []

          # Art.8: Must identify and classify all ICT assets
          asset_docs = [
              "docs/asset-register.md", "docs/ict-assets.md",
              "docs/asset-inventory.md", "inventory/",
              "docs/asset-classification.md"
          ]
          if not any(os.path.exists(p) for p in asset_docs):
              violations.append({
                  "article": "DORA-Art8", "severity": "HIGH",
                  "violation": "No ICT asset register or inventory documentation found",
                  "requirement": "Art.8 requires identification and classification of all ICT assets",
                  "remediation": "Create docs/asset-register.md listing all ICT assets with classification (critical/non-critical)"
              })

          # Art.8: Data flows and dependencies must be mapped
          data_flow_docs = [
              "docs/data-flow.md", "docs/architecture.md",
              "docs/system-architecture.md", "docs/data-mapping.md",
              "architecture/", "docs/architecture/"
          ]
          if not any(os.path.exists(p) for p in data_flow_docs):
              violations.append({
                  "article": "DORA-Art8", "severity": "MEDIUM",
                  "violation": "No data flow or system architecture documentation",
                  "requirement": "Art.8 requires mapping of information flows and system dependencies",
                  "remediation": "Create docs/architecture.md with system diagram and data flows"
              })

          with open("${{ env.REPORT_DIR }}/dora-art8-assets.json", "w") as f:
              json.dump(violations, f, indent=2)
          print(f"[DORA-Art8] {len(violations)} violations")
          EOF

      - name: DORA Art.9 — ICT Security Policies
        run: |
          python3 << 'EOF'
          import os, json

          violations = []

          required_policies = {
              "Information Security Policy":   ["SECURITY.md", "docs/security-policy.md", "docs/infosec-policy.md"],
              "Access Control Policy":         ["docs/access-control.md", "docs/access-policy.md", "docs/iam-policy.md"],
              "Encryption Policy":             ["docs/encryption-policy.md", "docs/crypto-policy.md"],
              "Patch Management Policy":       ["docs/patch-management.md", "docs/vulnerability-management.md"],
              "Change Management Policy":      ["docs/change-management.md", "CHANGELOG.md"],
              "Logging & Monitoring Policy":   ["docs/logging-policy.md", "docs/monitoring.md"],
          }

          for policy_name, paths in required_policies.items():
              if not any(os.path.exists(p) for p in paths):
                  violations.append({
                      "article": "DORA-Art9", "severity": "HIGH",
                      "violation": f"Missing required policy: {policy_name}",
                      "requirement": "Art.9 requires documented ICT security policies and procedures",
                      "remediation": f"Create one of: {', '.join(paths)}"
                  })

          with open("${{ env.REPORT_DIR }}/dora-art9-policies.json", "w") as f:
              json.dump(violations, f, indent=2)
          print(f"[DORA-Art9] {len(violations)} violations")
          EOF

      - name: DORA Art.10 — Detection & Monitoring
        run: |
          python3 << 'EOF'
          import os, json, glob

          violations = []

          # Art.10: Automated anomaly detection and logging
          monitoring_tools = [
              "prometheus", "grafana", "datadog", "newrelic", "splunk",
              "elasticsearch", "kibana", "fluentd", "loki", "jaeger",
              "opentelemetry", "sentry", "dynatrace", "alertmanager"
          ]
          monitoring_found = False

          for root, dirs, files in os.walk("."):
              dirs[:] = [d for d in dirs if d not in [".git", "node_modules", ".venv"]]
              for fname in files:
                  if fname.endswith((".yaml", ".yml", ".json", ".toml", ".conf")):
                      try:
                          with open(os.path.join(root, fname)) as f:
                              content = f.read().lower()
                          if any(tool in content for tool in monitoring_tools):
                              monitoring_found = True
                              break
                      except:
                          pass
              if monitoring_found:
                  break

          if not monitoring_found:
              violations.append({
                  "article": "DORA-Art10", "severity": "HIGH",
                  "violation": "No monitoring/observability tooling detected in configuration files",
                  "requirement": "Art.10 requires continuous monitoring of ICT systems for anomalies",
                  "remediation": "Add monitoring config (Prometheus, Grafana, Datadog, or equivalent)"
              })

          # Check for structured logging
          log_patterns = ["structlog", "json_logging", "logging.config", '"level":', "log_level", "LOG_LEVEL"]
          log_found = any(
              any(pattern in open(os.path.join(root, f)).read() for pattern in log_patterns)
              for root, dirs, files in os.walk(".")
              for f in files if f.endswith((".py", ".js", ".ts", ".yaml", ".yml"))
              if os.path.getsize(os.path.join(root, f)) < 500000
              for _ in [True]
          ) if True else False

          try:
              log_found = False
              for root, dirs, files in os.walk("."):
                  dirs[:] = [d for d in dirs if d not in [".git", "node_modules", ".venv"]]
                  for fname in files:
                      if fname.endswith((".py", ".js", ".ts", ".yaml")):
                          try:
                              with open(os.path.join(root, fname)) as fh:
                                  content = fh.read()
                              if any(p in content for p in log_patterns):
                                  log_found = True
                                  break
                          except:
                              pass
                  if log_found:
                      break
          except:
              pass

          if not log_found:
              violations.append({
                  "article": "DORA-Art10", "severity": "MEDIUM",
                  "violation": "No structured logging configuration detected",
                  "requirement": "Art.10 requires audit logging of all significant ICT events",
                  "remediation": "Implement structured/JSON logging with log levels (INFO, WARN, ERROR)"
              })

          with open("${{ env.REPORT_DIR }}/dora-art10-monitoring.json", "w") as f:
              json.dump(violations, f, indent=2)
          print(f"[DORA-Art10] {len(violations)} violations")
          EOF

      - name: DORA Art.11 — Backup Recovery & Business Continuity
        run: |
          python3 << 'EOF'
          import os, json

          violations = []

          # Backup configuration
          backup_indicators = ["backup", "restore", "recovery", "disaster", "rto", "rpo"]
          backup_found = any(
              any(ind in fname.lower() for ind in backup_indicators)
              for root, dirs, files in os.walk(".")
              for fname in files
          )

          if not backup_found:
              violations.append({
                  "article": "DORA-Art11", "severity": "HIGH",
                  "violation": "No backup or disaster recovery documentation/configuration found",
                  "requirement": "Art.11 requires backup systems and documented recovery procedures",
                  "remediation": "Create docs/backup-recovery.md with backup schedule, RTO, RPO, and test results"
              })

          # RTO/RPO must be explicitly defined
          rto_found = False
          for root, dirs, files in os.walk("."):
              dirs[:] = [d for d in dirs if d not in [".git", "node_modules"]]
              for fname in files:
                  if fname.endswith((".md", ".yaml", ".yml", ".json", ".txt")):
                      try:
                          with open(os.path.join(root, fname)) as f:
                              content = f.read().lower()
                          if "rto" in content and "rpo" in content:
                              rto_found = True
                              break
                      except:
                          pass
              if rto_found:
                  break

          if not rto_found:
              violations.append({
                  "article": "DORA-Art11", "severity": "HIGH",
                  "violation": "RTO and RPO not defined in any documentation",
                  "requirement": "Art.11 requires defined Recovery Time Objective and Recovery Point Objective",
                  "remediation": "Define RTO and RPO in docs/backup-recovery.md (e.g., RTO: 4h, RPO: 1h)"
              })

          # Business continuity plan
          bcp_docs = ["docs/bcp.md", "docs/business-continuity.md", "BCP.md", "docs/continuity-plan.md"]
          if not any(os.path.exists(p) for p in bcp_docs):
              violations.append({
                  "article": "DORA-Art11", "severity": "HIGH",
                  "violation": "No Business Continuity Plan (BCP) documentation found",
                  "requirement": "Art.11 requires documented business continuity plan for ICT disruptions",
                  "remediation": "Create docs/business-continuity.md with failover procedures and contact list"
              })

          with open("${{ env.REPORT_DIR }}/dora-art11-bcdr.json", "w") as f:
              json.dump(violations, f, indent=2)
          print(f"[DORA-Art11] {len(violations)} violations")
          EOF

      - name: DORA Art.12 — Learning & Evolving (Post-incident reviews)
        run: |
          python3 << 'EOF'
          import os, json

          violations = []

          # Post-incident review documentation
          pir_docs = [
              "docs/post-incident-reviews/", "post-incident/",
              "docs/lessons-learned.md", "docs/incident-reviews.md",
              "retrospectives/"
          ]
          if not any(os.path.exists(p) for p in pir_docs):
              violations.append({
                  "article": "DORA-Art12", "severity": "MEDIUM",
                  "violation": "No post-incident review documentation directory found",
                  "requirement": "Art.12 requires post-incident reviews and continuous improvement",
                  "remediation": "Create docs/post-incident-reviews/ folder for incident retrospectives"
              })

          with open("${{ env.REPORT_DIR }}/dora-art12-learning.json", "w") as f:
              json.dump(violations, f, indent=2)
          print(f"[DORA-Art12] {len(violations)} violations")
          EOF

      - name: DORA Art.13 — ICT Business Continuity Testing
        run: |
          python3 << 'EOF'
          import os, json, glob

          violations = []

          # Check for DR/BCP test records
          test_indicators = ["dr-test", "bcp-test", "failover-test", "recovery-test", "continuity-test"]
          test_found = any(
              any(ind in fname.lower() for ind in test_indicators)
              for root, dirs, files in os.walk(".")
              for fname in files
          )

          if not test_found:
              violations.append({
                  "article": "DORA-Art13", "severity": "HIGH",
                  "violation": "No Business Continuity or DR test records found",
                  "requirement": "Art.13 requires regular testing of ICT continuity plans",
                  "remediation": "Document annual BCP/DR test results in docs/post-incident-reviews/"
              })

          with open("${{ env.REPORT_DIR }}/dora-art13-bcp-testing.json", "w") as f:
              json.dump(violations, f, indent=2)
          print(f"[DORA-Art13] {len(violations)} violations")
          EOF

      - uses: actions/upload-artifact@v4
        with:
          name: dora-chapter2-reports
          path: ${{ env.REPORT_DIR }}/dora-art*.json


  # ── DORA Chapter 3: Incident Reporting ─────────────────────
  dora-chapter3-incident-reporting:
    name: "DORA Ch.III — Incident Management (Art.17–23)"
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: DORA Art.17 — ICT Incident Management Process
        run: |
          python3 << 'EOF'
          import os, json

          violations = []
          mkdir = lambda: os.makedirs("${{ env.REPORT_DIR }}", exist_ok=True) or None
          mkdir()

          # Incident classification criteria
          incident_docs = [
              "INCIDENT_RESPONSE.md", "docs/incident-response.md",
              "docs/incident-management.md", "ops/incident-response.md",
              ".github/SECURITY.md"
          ]
          if not any(os.path.exists(p) for p in incident_docs):
              violations.append({
                  "article": "DORA-Art17", "severity": "CRITICAL",
                  "violation": "No ICT incident management process documentation found",
                  "requirement": "Art.17 requires defined process for logging, classifying, and handling ICT incidents",
                  "remediation": "Create INCIDENT_RESPONSE.md with classification criteria and escalation procedures"
              })
          else:
              # Check it contains required elements
              for p in incident_docs:
                  if os.path.exists(p):
                      with open(p) as f:
                          content = f.read().lower()
                      required_elements = ["classif", "escalat", "contact", "report"]
                      missing = [e for e in required_elements if e not in content]
                      if missing:
                          violations.append({
                              "article": "DORA-Art17", "severity": "MEDIUM",
                              "violation": f"Incident response doc missing sections: {missing}",
                              "requirement": "Art.17 requires classification criteria, escalation procedures, and contact details",
                              "remediation": f"Update {p} to include: classification criteria, escalation matrix, contact list"
                          })
                      break

          with open("${{ env.REPORT_DIR }}/dora-art17-incident-mgmt.json", "w") as f:
              json.dump(violations, f, indent=2)
          print(f"[DORA-Art17] {len(violations)} violations")
          EOF

      - name: DORA Art.18 — Incident Classification
        run: |
          python3 << 'EOF'
          import os, json

          violations = []
          os.makedirs("${{ env.REPORT_DIR }}", exist_ok=True)

          # Must define major vs minor incident criteria
          classification_keywords = [
              "major incident", "significant incident",
              "classification", "severity level",
              "impact assessment", "users affected"
          ]

          found = False
          for root, dirs, files in os.walk("."):
              dirs[:] = [d for d in dirs if d not in [".git", "node_modules"]]
              for fname in files:
                  if fname.endswith(".md"):
                      try:
                          with open(os.path.join(root, fname)) as f:
                              content = f.read().lower()
                          if sum(1 for kw in classification_keywords if kw in content) >= 2:
                              found = True
                              break
                      except:
                          pass
              if found:
                  break

          if not found:
              violations.append({
                  "article": "DORA-Art18", "severity": "HIGH",
                  "violation": "No incident classification criteria defined (major vs minor incident thresholds)",
                  "requirement": "Art.18 requires documented criteria for classifying incidents as major or minor",
                  "remediation": "Add incident classification matrix to INCIDENT_RESPONSE.md (criteria: users affected, downtime, financial impact)"
              })

          with open("${{ env.REPORT_DIR }}/dora-art18-classification.json", "w") as f:
              json.dump(violations, f, indent=2)
          print(f"[DORA-Art18] {len(violations)} violations")
          EOF

      - name: DORA Art.19 — Major Incident Reporting (Regulatory)
        run: |
          python3 << 'EOF'
          import os, json

          violations = []
          os.makedirs("${{ env.REPORT_DIR }}", exist_ok=True)

          # Must have regulatory reporting procedures (4h initial, 72h detailed, 1-month final)
          reporting_keywords = ["4 hour", "4-hour", "72 hour", "72-hour", "initial notification",
                                "competent authorit", "regulatory report", "eba", "esma", "eiopa",
                                "1 month", "final report", "intermediate report"]

          found = False
          for root, dirs, files in os.walk("."):
              dirs[:] = [d for d in dirs if d not in [".git", "node_modules"]]
              for fname in files:
                  if fname.endswith(".md"):
                      try:
                          with open(os.path.join(root, fname)) as f:
                              content = f.read().lower()
                          if sum(1 for kw in reporting_keywords if kw in content) >= 2:
                              found = True
                              break
                      except:
                          pass
              if found:
                  break

          if not found:
              violations.append({
                  "article": "DORA-Art19", "severity": "CRITICAL",
                  "violation": "No regulatory incident reporting timeline documented (4h/72h/1-month rule)",
                  "requirement": "Art.19 requires reporting major incidents to competent authority within 4h (initial), 72h (intermediate), 1 month (final)",
                  "remediation": "Add regulatory reporting timeline to INCIDENT_RESPONSE.md — EBA/ESMA/EIOPA notification procedures"
              })

          with open("${{ env.REPORT_DIR }}/dora-art19-regulatory-reporting.json", "w") as f:
              json.dump(violations, f, indent=2)
          print(f"[DORA-Art19] {len(violations)} violations")
          EOF

      - uses: actions/upload-artifact@v4
        with:
          name: dora-chapter3-reports
          path: ${{ env.REPORT_DIR }}/dora-art1*.json


  # ── DORA Chapter 4: Resilience Testing ─────────────────────
  dora-chapter4-resilience-testing:
    name: "DORA Ch.IV — Resilience Testing (Art.24–27)"
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Install security tools
        run: |
          sudo apt-get update -qq
          wget -qO- https://aquasecurity.github.io/trivy-repo/deb/public.key | sudo apt-key add -
          echo "deb https://aquasecurity.github.io/trivy-repo/deb generic main" | sudo tee /etc/apt/sources.list.d/trivy.list
          sudo apt-get update -qq && sudo apt-get install -y trivy jq
          mkdir -p ${{ env.REPORT_DIR }}

      - name: DORA Art.24 — Vulnerability Assessment Programme
        run: |
          echo "=== DORA Art.24 — Vulnerability Assessments ==="
          # Trivy scans for vulnerabilities — this IS the Art.24 check
          trivy fs . \
            --format json \
            --output ${{ env.REPORT_DIR }}/dora-art24-vulnerabilities.json \
            --severity CRITICAL,HIGH,MEDIUM \
            --exit-code 0

          python3 << 'EOF'
          import json, os

          with open("${{ env.REPORT_DIR }}/dora-art24-vulnerabilities.json") as f:
              data = json.load(f)

          violations = []
          for result in data.get("Results", []):
              for vuln in result.get("Vulnerabilities", []):
                  if vuln.get("Severity") in ["CRITICAL", "HIGH"]:
                      violations.append({
                          "article": "DORA-Art24",
                          "severity": vuln["Severity"],
                          "violation": f"Unpatched {vuln['Severity']} vulnerability: {vuln.get('VulnerabilityID','N/A')} in {vuln.get('PkgName','N/A')}",
                          "cvss_score": vuln.get("CVSS", {}).get("nvd", {}).get("V3Score", "N/A"),
                          "fix_version": vuln.get("FixedVersion", "No fix available"),
                          "requirement": "Art.24 requires systematic vulnerability assessment and prompt remediation"
                      })

          # Also check: is there a scheduled vulnerability scan programme?
          scan_programme_docs = [
              "docs/vulnerability-management.md", "docs/patch-management.md",
              "docs/security-testing.md", "docs/vuln-programme.md"
          ]
          if not any(os.path.exists(p) for p in scan_programme_docs):
              violations.append({
                  "article": "DORA-Art24", "severity": "MEDIUM",
                  "violation": "No vulnerability management programme documentation found",
                  "requirement": "Art.24 requires a defined vulnerability assessment programme",
                  "remediation": "Create docs/vulnerability-management.md with scan schedule and SLA for remediation"
              })

          with open("${{ env.REPORT_DIR }}/dora-art24-vuln-summary.json", "w") as f:
              json.dump(violations, f, indent=2)
          print(f"[DORA-Art24] {len(violations)} vulnerabilities/violations")
          EOF

      - name: DORA Art.25 — Digital Operational Resilience Testing
        run: |
          python3 << 'EOF'
          import os, json

          violations = []
          os.makedirs("${{ env.REPORT_DIR }}", exist_ok=True)

          # Penetration test records
          pentest_keywords = ["pentest", "penetration", "red-team", "security-assessment", "tlpt"]
          pentest_found = any(
              any(kw in fname.lower() for kw in pentest_keywords)
              for root, dirs, files in os.walk(".")
              for fname in files
          )

          if not pentest_found:
              violations.append({
                  "article": "DORA-Art25", "severity": "HIGH",
                  "violation": "No penetration test or security assessment records found",
                  "requirement": "Art.25 requires annual resilience testing including threat-led penetration testing for significant entities",
                  "remediation": "Document most recent pentest in docs/pentest-YYYY.md and maintain test schedule"
              })

          # Resilience/chaos testing
          chaos_keywords = ["chaos", "gremlin", "litmus", "fault-inject", "game-day", "gameday", "stress-test"]
          chaos_found = any(
              any(kw in fname.lower() for kw in chaos_keywords)
              for root, dirs, files in os.walk(".")
              for fname in files
          )

          if not chaos_found:
              violations.append({
                  "article": "DORA-Art25", "severity": "MEDIUM",
                  "violation": "No chaos engineering or operational resilience testing found",
                  "requirement": "Art.25 requires testing digital operational resilience (disruption simulation)",
                  "remediation": "Implement chaos engineering tests (LitmusChaos, Gremlin) or document game-day exercises"
              })

          with open("${{ env.REPORT_DIR }}/dora-art25-resilience.json", "w") as f:
              json.dump(violations, f, indent=2)
          print(f"[DORA-Art25] {len(violations)} violations")
          EOF

      - name: DORA Art.26 — TLPT (Threat-Led Penetration Testing)
        run: |
          python3 << 'EOF'
          import os, json

          violations = []
          os.makedirs("${{ env.REPORT_DIR }}", exist_ok=True)

          # TLPT is required for 'significant' financial entities
          # Check for TLPT documentation
          tlpt_docs = ["docs/tlpt.md", "docs/threat-led-pentest.md", "security/tlpt/"]
          if not any(os.path.exists(p) for p in tlpt_docs):
              violations.append({
                  "article": "DORA-Art26", "severity": "MEDIUM",
                  "violation": "No TLPT (Threat-Led Penetration Testing) documentation found",
                  "requirement": "Art.26 requires TLPT for significant financial entities every 3 years",
                  "remediation": "If in-scope (significant entity): document TLPT programme in docs/tlpt.md. If not in-scope: document exemption rationale"
              })

          with open("${{ env.REPORT_DIR }}/dora-art26-tlpt.json", "w") as f:
              json.dump(violations, f, indent=2)
          print(f"[DORA-Art26] {len(violations)} violations")
          EOF

      - uses: actions/upload-artifact@v4
        with:
          name: dora-chapter4-reports
          path: ${{ env.REPORT_DIR }}/dora-art2*.json


  # ── DORA Chapter 5: Third-Party Risk ───────────────────────
  dora-chapter5-third-party:
    name: "DORA Ch.V — Third-Party ICT Risk (Art.28–44)"
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: DORA Art.28 — Third-Party Risk Policy
        run: |
          python3 << 'EOF'
          import os, json

          violations = []
          os.makedirs("${{ env.REPORT_DIR }}", exist_ok=True)

          # Third-party risk policy
          policy_docs = [
              "docs/third-party-risk-policy.md", "docs/vendor-risk-policy.md",
              "docs/tprm-policy.md", "docs/third-party-policy.md"
          ]
          if not any(os.path.exists(p) for p in policy_docs):
              violations.append({
                  "article": "DORA-Art28", "severity": "HIGH",
                  "violation": "No Third-Party ICT Risk Management Policy found",
                  "requirement": "Art.28 requires documented policy for managing ICT third-party risks",
                  "remediation": "Create docs/third-party-risk-policy.md covering: due diligence, risk assessment, exit strategy"
              })

          # Register of all ICT third-party providers (RoI DORA)
          register_docs = [
              "docs/third-party-register.md", "docs/vendor-register.md",
              "docs/ict-provider-register.md", "vendors.md",
              "docs/ict-contracts.md"
          ]
          if not any(os.path.exists(p) for p in register_docs):
              violations.append({
                  "article": "DORA-Art28", "severity": "CRITICAL",
                  "violation": "No Register of Information (RoI) for ICT third-party providers found",
                  "requirement": "Art.28 + ESA mandate requires Register of Information submitted to national authority by April 2025",
                  "remediation": "Create docs/third-party-register.md listing ALL ICT vendors with: service, data processed, contract ref, criticality"
              })

          with open("${{ env.REPORT_DIR }}/dora-art28-tpr.json", "w") as f:
              json.dump(violations, f, indent=2)
          print(f"[DORA-Art28] {len(violations)} violations")
          EOF

      - name: DORA Art.29 — Preliminary Assessment of ICT Third-Party Providers
        run: |
          python3 << 'EOF'
          import os, json

          violations = []
          os.makedirs("${{ env.REPORT_DIR }}", exist_ok=True)

          # Due diligence / risk assessment records for vendors
          due_diligence_docs = [
              "docs/vendor-assessments/", "docs/third-party-assessments/",
              "docs/due-diligence/", "security/vendor-reviews/"
          ]
          if not any(os.path.exists(p) for p in due_diligence_docs):
              violations.append({
                  "article": "DORA-Art29", "severity": "HIGH",
                  "violation": "No ICT third-party due diligence or risk assessment records found",
                  "requirement": "Art.29 requires preliminary assessment of ICT third-party providers before engagement",
                  "remediation": "Create docs/vendor-assessments/ with individual vendor risk assessments"
              })

          with open("${{ env.REPORT_DIR }}/dora-art29-due-diligence.json", "w") as f:
              json.dump(violations, f, indent=2)
          print(f"[DORA-Art29] {len(violations)} violations")
          EOF

      - name: DORA Art.30 — Key Contractual Provisions
        run: |
          python3 << 'EOF'
          import os, json

          violations = []
          os.makedirs("${{ env.REPORT_DIR }}", exist_ok=True)

          # Contract documentation with required DORA clauses
          contract_docs = ["docs/ict-contracts.md", "docs/vendor-contracts.md", "contracts/"]
          contract_found = any(os.path.exists(p) for p in contract_docs)

          if not contract_found:
              violations.append({
                  "article": "DORA-Art30", "severity": "HIGH",
                  "violation": "No ICT contract documentation with DORA-required clauses found",
                  "requirement": "Art.30 requires contracts to include: service description, audit rights, SLA, incident notification, exit plan",
                  "remediation": "Create docs/ict-contracts.md listing contracts with verification that DORA clauses are included"
              })
          else:
              # Check that contracts mention required elements
              required_clauses = ["sla", "audit", "exit", "incident", "subcontract", "data location"]
              for p in contract_docs:
                  if os.path.exists(p) and os.path.isfile(p):
                      with open(p) as f:
                          content = f.read().lower()
                      missing = [c for c in required_clauses if c not in content]
                      if missing:
                          violations.append({
                              "article": "DORA-Art30", "severity": "MEDIUM",
                              "violation": f"Contract documentation missing required DORA clauses: {missing}",
                              "requirement": "Art.30 requires specific contract clauses: SLA, audit rights, exit strategy, incident notification",
                              "remediation": f"Update {p} to include sections for: {', '.join(missing)}"
                          })
                      break

          # Exit strategy documentation
          exit_docs = ["docs/exit-strategy.md", "docs/vendor-exit-plan.md", "docs/exit-plan.md"]
          if not any(os.path.exists(p) for p in exit_docs):
              violations.append({
                  "article": "DORA-Art30", "severity": "MEDIUM",
                  "violation": "No ICT vendor exit strategy documentation found",
                  "requirement": "Art.30 requires documented exit strategy for each critical ICT provider",
                  "remediation": "Create docs/exit-strategy.md covering: transition plan, data portability, alternative providers"
              })

          with open("${{ env.REPORT_DIR }}/dora-art30-contracts.json", "w") as f:
              json.dump(violations, f, indent=2)
          print(f"[DORA-Art30] {len(violations)} violations")
          EOF

      - name: DORA Art.44 — Information Sharing Arrangements
        run: |
          python3 << 'EOF'
          import os, json

          violations = []
          os.makedirs("${{ env.REPORT_DIR }}", exist_ok=True)

          # Information sharing (ISAC membership, threat intelligence)
          sharing_docs = [
              "docs/threat-intelligence.md", "docs/information-sharing.md",
              "docs/isac-membership.md", "docs/cyber-threat-intel.md"
          ]
          if not any(os.path.exists(p) for p in sharing_docs):
              violations.append({
                  "article": "DORA-Art44", "severity": "LOW",
                  "violation": "No cyber threat intelligence or information sharing arrangement documented",
                  "requirement": "Art.44 encourages financial entities to share cyber threat intelligence",
                  "remediation": "Document participation in FS-ISAC, national CERT, or sector threat sharing group"
              })

          with open("${{ env.REPORT_DIR }}/dora-art44-sharing.json", "w") as f:
              json.dump(violations, f, indent=2)
          print(f"[DORA-Art44] {len(violations)} violations")
          EOF

      - uses: actions/upload-artifact@v4
        with:
          name: dora-chapter5-reports
          path: ${{ env.REPORT_DIR }}/dora-art*.json


  # ============================================================
  # JOB 3: NIS2 Full Checks
  # ============================================================
  nis2-compliance:
    name: "NIS2 — Art.21 + Art.23 Full Check"
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Install tools
        run: |
          sudo apt-get update -qq
          wget -qO- https://aquasecurity.github.io/trivy-repo/deb/public.key | sudo apt-key add -
          echo "deb https://aquasecurity.github.io/trivy-repo/deb generic main" | sudo tee /etc/apt/sources.list.d/trivy.list
          sudo apt-get update -qq && sudo apt-get install -y trivy jq
          pip install semgrep --quiet
          mkdir -p ${{ env.REPORT_DIR }}

      - name: NIS2 Art.21 — CVE Vulnerability Scan
        run: |
          trivy fs . \
            --format json \
            --output ${{ env.REPORT_DIR }}/nis2-cve.json \
            --severity CRITICAL,HIGH,MEDIUM \
            --exit-code 0

      - name: NIS2 Art.21 — Secret Detection & Cryptography
        run: |
          trivy fs . --scanners secret \
            --format json \
            --output ${{ env.REPORT_DIR }}/nis2-secrets.json \
            --exit-code 0

          python3 << 'EOF'
          import os, json, re

          violations = []

          patterns = {
              "hardcoded_password":  r'(?i)(password|passwd|pwd)\s*[=:]\s*["\'][^"\']{6,}["\']',
              "hardcoded_api_key":   r'(?i)(api[_-]?key|apikey)\s*[=:]\s*["\'][^"\']{8,}["\']',
              "hardcoded_token":     r'(?i)(token|secret)\s*[=:]\s*["\'][^"\']{8,}["\']',
              "aws_access_key":      r'AKIA[0-9A-Z]{16}',
              "private_key_header":  r'-----BEGIN (RSA |EC |OPENSSH )?PRIVATE KEY-----',
              "tls_disabled":        r'(?i)(tls|ssl)\s*:\s*(false|disabled|0)',
              "insecure_http":       r'(?i)http://(?!localhost|127\.0\.0\.1)',
              "weak_cipher_md5":     r'(?i)(md5|sha1|des)\s*[=(]',
          }

          exclude_dirs = {".git", "node_modules", ".venv", "vendor", "dist", "build"}

          for root, dirs, files in os.walk("."):
              dirs[:] = [d for d in dirs if d not in exclude_dirs]
              for fname in files:
                  if not fname.endswith((".py", ".js", ".ts", ".go", ".java", ".rs",
                                         ".rb", ".php", ".yaml", ".yml", ".env",
                                         ".conf", ".toml", ".kt", ".scala", ".c",
                                         ".cpp", ".h", ".cu", ".cuh")):
                      continue
                  fpath = os.path.join(root, fname)
                  try:
                      with open(fpath, errors="ignore") as f:
                          content = f.read()
                      for pname, pattern in patterns.items():
                          matches = re.findall(pattern, content)
                          if matches:
                              violations.append({
                                  "article": "NIS2-Art21",
                                  "severity": "CRITICAL" if "key" in pname or "password" in pname or "private_key" in pname else "HIGH",
                                  "violation": f"{pname} detected in {fpath}",
                                  "requirement": "NIS2 Art.21(2)(h)/(i) — cryptography and access control",
                                  "remediation": f"Remove hardcoded {pname.replace('_', ' ')} from source code. Use environment variables or secret manager."
                              })
                  except:
                      pass

          with open("${{ env.REPORT_DIR }}/nis2-crypto-access.json", "w") as f:
              json.dump(violations, f, indent=2)
          print(f"[NIS2-Art21-Crypto] {len(violations)} violations found")
          EOF

      - name: NIS2 Art.21 — Security Policy & Documentation
        run: |
          python3 << 'EOF'
          import os, json

          violations = []

          checks = {
              "NIS2-Art21(a) Risk Analysis Policy":    ["SECURITY.md", ".github/SECURITY.md", "docs/security-policy.md"],
              "NIS2-Art21(b) Incident Handling":       ["INCIDENT_RESPONSE.md", "docs/incident-response.md"],
              "NIS2-Art21(c) Business Continuity":     ["docs/bcp.md", "docs/business-continuity.md"],
              "NIS2-Art21(d) Supply Chain Security":   ["docs/supply-chain.md", "docs/third-party-security.md"],
              "NIS2-Art21(e) Vulnerability Handling":  ["docs/vulnerability-management.md", ".github/dependabot.yml", ".renovaterc.json"],
              "NIS2-Art21(f) Security Testing":        ["docs/security-testing.md", "docs/pentest-*.md"],
              "NIS2-Art21(g) Cybersecurity Training":  ["docs/security-training.md", "docs/training.md"],
          }

          for req_name, paths in checks.items():
              import glob as g
              found = any(
                  os.path.exists(p) if '*' not in p else bool(g.glob(p))
                  for p in paths
              )
              if not found:
                  violations.append({
                      "article": "NIS2-Art21",
                      "severity": "HIGH",
                      "violation": f"Missing: {req_name}",
                      "remediation": f"Create one of: {', '.join(paths)}"
                  })

          with open("${{ env.REPORT_DIR }}/nis2-art21-docs.json", "w") as f:
              json.dump(violations, f, indent=2)
          print(f"[NIS2-Art21-Docs] {len(violations)} violations")
          EOF

      - name: NIS2 Art.23 — Incident Reporting (72h rule)
        run: |
          python3 << 'EOF'
          import os, json

          violations = []
          reporting_keywords = ["72 hour", "72h", "24 hour", "initial notification",
                                "early warning", "competent authority", "ncsc", "bsi",
                                "national authority", "cert-eu"]

          found = False
          for root, dirs, files in os.walk("."):
              dirs[:] = [d for d in dirs if d not in [".git", "node_modules"]]
              for fname in files:
                  if fname.endswith(".md"):
                      try:
                          with open(os.path.join(root, fname)) as f:
                              content = f.read().lower()
                          if sum(1 for kw in reporting_keywords if kw in content) >= 2:
                              found = True
                              break
                      except:
                          pass
              if found:
                  break

          if not found:
              violations.append({
                  "article": "NIS2-Art23", "severity": "CRITICAL",
                  "violation": "No NIS2 72-hour incident reporting procedure documented",
                  "requirement": "Art.23 requires early warning within 24h, notification within 72h to national authority (NCSC-NL/BSI)",
                  "remediation": "Add NIS2 reporting timeline to INCIDENT_RESPONSE.md: 24h early warning + 72h detailed report to national authority"
              })

          with open("${{ env.REPORT_DIR }}/nis2-art23-reporting.json", "w") as f:
              json.dump(violations, f, indent=2)
          print(f"[NIS2-Art23] {len(violations)} violations")
          EOF

      - uses: actions/upload-artifact@v4
        with:
          name: nis2-reports
          path: ${{ env.REPORT_DIR }}/nis2-*.json


  # ============================================================
  # JOB 4: Multi-Language Code Scanning
  # ============================================================
  multilang-code-scan:
    name: "Multi-Language Security Scan (.py .js .ts .go .java .rs .rb .php .kt .scala .c .cpp)"
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Install language-specific scanners
        run: |
          pip install semgrep bandit safety --quiet
          mkdir -p ${{ env.REPORT_DIR }}

      - name: Python Security Scan (Bandit + Semgrep)
        run: |
          # Bandit — Python-specific security scanner
          if find . -name "*.py" -not -path "*/node_modules/*" -not -path "*/.venv/*" | head -1 | grep -q .; then
            bandit -r . \
              --exclude ./.venv,./node_modules,./dist \
              -f json \
              -o ${{ env.REPORT_DIR }}/lang-python-bandit.json \
              --quiet || true
            echo "[Python/Bandit] Scan complete"
          else
            echo '{"results": [], "errors": []}' > ${{ env.REPORT_DIR }}/lang-python-bandit.json
            echo "[Python] No .py files found"
          fi

      - name: Multi-Language Semgrep SAST
        run: |
          # Semgrep covers: Python, JS, TS, Go, Java, Ruby, PHP, Rust, Kotlin, Scala, C/C++
          semgrep \
            --config=auto \
            --json \
            --output=${{ env.REPORT_DIR }}/lang-semgrep-all.json \
            --exclude=node_modules \
            --exclude=.venv \
            --exclude=vendor \
            --exclude=dist \
            --no-git-ignore \
            . || true
          echo "[Semgrep] Multi-language SAST complete"

      - name: Python-specific DORA/NIS2 compliance checks
        run: |
          python3 << 'EOF'
          import os, json, ast, re

          violations = []
          py_files = []

          for root, dirs, files in os.walk("."):
              dirs[:] = [d for d in dirs if d not in [".git", "node_modules", ".venv", "dist"]]
              for fname in files:
                  if fname.endswith(".py"):
                      py_files.append(os.path.join(root, fname))

          print(f"[Python] Scanning {len(py_files)} .py files")

          for fpath in py_files:
              try:
                  with open(fpath, errors="ignore") as f:
                      content = f.read()

                  # Check for unsafe deserialization (pickle — security risk)
                  if "pickle.load" in content or "pickle.loads" in content:
                      violations.append({
                          "article": "NIS2-Art21", "severity": "HIGH",
                          "language": "Python",
                          "violation": f"Unsafe pickle deserialization in {fpath}",
                          "requirement": "NIS2 Art.21 — insecure deserialization can lead to RCE",
                          "remediation": "Replace pickle with json or a safer serialization format"
                      })

                  # Check for eval() / exec() on external input
                  if re.search(r'\beval\s*\(|exec\s*\(', content):
                      violations.append({
                          "article": "NIS2-Art21", "severity": "HIGH",
                          "language": "Python",
                          "violation": f"eval()/exec() usage in {fpath} — potential code injection",
                          "remediation": "Remove eval()/exec() — use safe alternatives"
                      })

                  # Check for SQL injection risk
                  if re.search(r'execute\s*\(\s*["\'].*%s.*["\']|format\s*\(.*SQL|f["\'].*SELECT.*\{', content):
                      violations.append({
                          "article": "NIS2-Art21", "severity": "HIGH",
                          "language": "Python",
                          "violation": f"Potential SQL injection pattern in {fpath}",
                          "remediation": "Use parameterized queries instead of string formatting"
                      })

                  # Check for missing exception handling in critical sections
                  if "except:" in content and "pass" in content:
                      violations.append({
                          "article": "DORA-Art10", "severity": "MEDIUM",
                          "language": "Python",
                          "violation": f"Bare except: pass (swallowing exceptions) in {fpath}",
                          "requirement": "DORA Art.10 requires adequate error logging for incident detection",
                          "remediation": "Replace bare except/pass with specific exception handling and logging"
                      })

                  # Check for hardcoded DEBUG=True
                  if re.search(r'DEBUG\s*=\s*True', content):
                      violations.append({
                          "article": "NIS2-Art21", "severity": "HIGH",
                          "language": "Python",
                          "violation": f"DEBUG=True hardcoded in {fpath} — may expose sensitive info in production",
                          "remediation": "Use environment variable: DEBUG = os.environ.get('DEBUG', 'False') == 'True'"
                      })

              except Exception as e:
                  pass

          with open("${{ env.REPORT_DIR }}/lang-python-compliance.json", "w") as f:
              json.dump(violations, f, indent=2)
          print(f"[Python-Compliance] {len(violations)} violations in {len(py_files)} files")
          EOF

      - name: JavaScript/TypeScript compliance checks
        run: |
          python3 << 'EOF'
          import os, json, re

          violations = []
          js_files = []

          for root, dirs, files in os.walk("."):
              dirs[:] = [d for d in dirs if d not in [".git", "node_modules", ".venv", "dist", "build"]]
              for fname in files:
                  if fname.endswith((".js", ".ts", ".jsx", ".tsx", ".mjs")):
                      js_files.append(os.path.join(root, fname))

          print(f"[JS/TS] Scanning {len(js_files)} files")

          for fpath in js_files:
              try:
                  with open(fpath, errors="ignore") as f:
                      content = f.read()

                  # eval() usage
                  if re.search(r'\beval\s*\(', content):
                      violations.append({
                          "article": "NIS2-Art21", "severity": "HIGH",
                          "language": "JavaScript/TypeScript",
                          "violation": f"eval() usage in {fpath}",
                          "remediation": "Remove eval() — use safer alternatives like JSON.parse()"
                      })

                  # dangerouslySetInnerHTML (XSS risk)
                  if "dangerouslySetInnerHTML" in content:
                      violations.append({
                          "article": "NIS2-Art21", "severity": "HIGH",
                          "language": "JavaScript/TypeScript",
                          "violation": f"dangerouslySetInnerHTML usage in {fpath} — XSS risk",
                          "remediation": "Sanitize HTML content with DOMPurify before using dangerouslySetInnerHTML"
                      })

                  # document.write()
                  if re.search(r'document\.write\s*\(', content):
                      violations.append({
                          "article": "NIS2-Art21", "severity": "MEDIUM",
                          "language": "JavaScript/TypeScript",
                          "violation": f"document.write() in {fpath} — DOM-based XSS risk",
                          "remediation": "Use safe DOM manipulation methods instead"
                      })

                  # Insecure cryptography
                  if re.search(r'MD5|sha1\(|createHash\(["\']md5["\']|createHash\(["\']sha1["\']', content):
                      violations.append({
                          "article": "NIS2-Art21", "severity": "HIGH",
                          "language": "JavaScript/TypeScript",
                          "violation": f"Weak cryptographic hash (MD5/SHA1) in {fpath}",
                          "remediation": "Replace with SHA-256 or bcrypt/argon2 for passwords"
                      })

              except:
                  pass

          with open("${{ env.REPORT_DIR }}/lang-js-compliance.json", "w") as f:
              json.dump(violations, f, indent=2)
          print(f"[JS/TS] {len(violations)} violations in {len(js_files)} files")
          EOF

      - name: Go, Java, Rust, C/C++ compliance checks
        run: |
          python3 << 'EOF'
          import os, json, re

          violations = []

          lang_checks = {
              ".go": {
                  "patterns": {
                      "sql_injection": r'fmt\.Sprintf\(".*SELECT|fmt\.Sprintf\(".*INSERT',
                      "insecure_rng":  r'math/rand',  # should use crypto/rand
                      "weak_tls":      r'InsecureSkipVerify:\s*true',
                      "hardcoded_creds": r'(?i)(password|secret|apikey)\s*:=\s*"[^"]{6,}"',
                  },
                  "lang": "Go"
              },
              ".java": {
                  "patterns": {
                      "sql_injection":     r'executeQuery\s*\(\s*".*\+|Statement\s+stmt',
                      "deserialization":   r'ObjectInputStream|readObject\s*\(',
                      "weak_crypto":       r'MD5|SHA-1|DES\b',
                      "hardcoded_creds":   r'(?i)(password|secret)\s*=\s*"[^"]{6,}"',
                      "xxe_injection":     r'XMLInputFactory|DocumentBuilder',
                  },
                  "lang": "Java"
              },
              ".rs": {
                  "patterns": {
                      "unsafe_block":      r'\bunsafe\s*\{',
                      "hardcoded_creds":   r'(?i)(password|secret|api_key)\s*=\s*"[^"]{6,}"',
                  },
                  "lang": "Rust"
              },
              ".c": {
                  "patterns": {
                      "buffer_overflow":   r'\bgets\s*\(|\bstrcpy\s*\(|\bsprintf\s*\(',
                      "format_string":     r'printf\s*\(\s*[^"]\w+\s*\)',
                  },
                  "lang": "C"
              },
              ".cpp": {
                  "patterns": {
                      "buffer_overflow":   r'\bgets\s*\(|\bstrcpy\s*\(|\bstrcmp\s*\(',
                      "deprecated_cast":   r'reinterpret_cast',
                  },
                  "lang": "C++"
              },
              ".rb": {
                  "patterns": {
                      "sql_injection":     r'\.where\s*\(\s*".*#\{|\.find_by_sql\s*\(".*#\{',
                      "code_injection":    r'\beval\s*\(',
                  },
                  "lang": "Ruby"
              },
              ".php": {
                  "patterns": {
                      "sql_injection":     r'mysql_query\s*\(|mysqli_query.*\.\s*\$',
                      "code_injection":    r'\beval\s*\(|\bexec\s*\(',
                      "xss":              r'echo\s+\$_(GET|POST|REQUEST)',
                  },
                  "lang": "PHP"
              },
          }

          for ext, config in lang_checks.items():
              files_found = []
              for root, dirs, files in os.walk("."):
                  dirs[:] = [d for d in dirs if d not in [".git", "node_modules", ".venv", "vendor", "dist"]]
                  for fname in files:
                      if fname.endswith(ext):
                          files_found.append(os.path.join(root, fname))

              if not files_found:
                  continue

              print(f"[{config['lang']}] Scanning {len(files_found)} {ext} files")
              for fpath in files_found:
                  try:
                      with open(fpath, errors="ignore") as f:
                          content = f.read()
                      for pname, pattern in config["patterns"].items():
                          if re.search(pattern, content):
                              violations.append({
                                  "article": "NIS2-Art21",
                                  "severity": "HIGH",
                                  "language": config["lang"],
                                  "violation": f"{pname} in {fpath}",
                                  "remediation": f"Review and fix {pname.replace('_', ' ')} vulnerability"
                              })
                  except:
                      pass

          with open("${{ env.REPORT_DIR }}/lang-multilang-compliance.json", "w") as f:
              json.dump(violations, f, indent=2)
          print(f"[MultiLang] {len(violations)} total violations across all languages")
          EOF

      - uses: actions/upload-artifact@v4
        with:
          name: multilang-reports
          path: ${{ env.REPORT_DIR }}/lang-*.json


  # ============================================================
  # JOB 5: GPU/AI Code Compliance Scanning
  # NEW — CUDA, OpenCL, PyTorch, TF, ONNX, Jupyter
  # ============================================================
  gpu-ai-compliance:
    name: "GPU/AI Code Compliance — CUDA | OpenCL | PyTorch | TensorFlow | ONNX | Notebooks"
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Setup Python for AI scanning
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install AI/GPU scanning tools
        run: |
          pip install nbformat nbconvert safety --quiet
          mkdir -p ${{ env.REPORT_DIR }}

      - name: Scan CUDA Code (.cu / .cuh)
        run: |
          python3 << 'EOF'
          import os, json, re

          violations = []
          cuda_files = []

          for root, dirs, files in os.walk("."):
              dirs[:] = [d for d in dirs if d not in [".git", "node_modules"]]
              for fname in files:
                  if fname.endswith((".cu", ".cuh")):
                      cuda_files.append(os.path.join(root, fname))

          if not cuda_files:
              print("[CUDA] No .cu/.cuh files found — skipping GPU scan")
          else:
              print(f"[CUDA] Scanning {len(cuda_files)} CUDA files")

          for fpath in cuda_files:
              try:
                  with open(fpath, errors="ignore") as f:
                      content = f.read()

                  # CUDA-specific security patterns
                  cuda_checks = {
                      "unbounded_kernel_launch": (
                          r'<<<\s*\w+\s*,\s*\w+\s*>>>',
                          "MEDIUM",
                          "CUDA kernel launch without bounds checking — may cause device overflow",
                          "Add cudaDeviceSynchronize() and check cudaGetLastError() after kernel launches"
                      ),
                      "missing_error_check": (
                          r'cudaMalloc|cudaMemcpy|cudaLaunchKernel',
                          "HIGH",
                          "CUDA API call without error checking",
                          "Wrap all CUDA calls with error check macro: #define CUDA_CHECK(call) {...}"
                      ),
                      "unencrypted_gpu_memory": (
                          r'cudaMallocManaged|__managed__',
                          "MEDIUM",
                          "Managed/unified GPU memory used — data may be accessible from CPU without access control",
                          "Document data sensitivity for managed memory regions — consider encryption for sensitive model weights"
                      ),
                      "hardcoded_gpu_index": (
                          r'cudaSetDevice\s*\(\s*[0-9]+\s*\)',
                          "LOW",
                          "Hardcoded GPU device index — not portable and may cause runtime failures",
                          "Use dynamic device selection with cudaGetDeviceCount()"
                      ),
                      "no_memory_free": (
                          r'cudaMalloc(?!.*cudaFree)',
                          "MEDIUM",
                          "cudaMalloc without matching cudaFree — GPU memory leak risk",
                          "Ensure every cudaMalloc has a corresponding cudaFree"
                      ),
                      "race_condition_shared_memory": (
                          r'__shared__(?!.*__syncthreads)',
                          "HIGH",
                          "__shared__ memory without __syncthreads() barrier — potential race condition",
                          "Add __syncthreads() after writing to shared memory"
                      ),
                  }

                  for check_name, (pattern, severity, violation_msg, remediation) in cuda_checks.items():
                      if re.search(pattern, content, re.DOTALL):
                          violations.append({
                              "article": "EU-AI-Act-Art13 / NIS2-Art21",
                              "severity": severity,
                              "language": "CUDA",
                              "file": fpath,
                              "violation": f"{violation_msg} in {os.path.basename(fpath)}",
                              "remediation": remediation,
                              "regulation_note": "GPU code running AI models is subject to EU AI Act Art.13 technical documentation and NIS2 Art.21 security requirements"
                          })

              except Exception as e:
                  print(f"Warning: Could not scan {fpath}: {e}")

          with open("${{ env.REPORT_DIR }}/gpu-cuda-violations.json", "w") as f:
              json.dump({"cuda_files_scanned": len(cuda_files), "violations": violations}, f, indent=2)
          print(f"[CUDA] {len(violations)} violations in {len(cuda_files)} files")
          EOF

      - name: Scan Python AI Frameworks (PyTorch / TensorFlow / JAX)
        run: |
          python3 << 'EOF'
          import os, json, re

          violations = []
          py_ai_files = []

          ai_import_patterns = re.compile(
              r'import torch|import tensorflow|import jax|import keras|from torch|from tensorflow|from jax'
          )

          for root, dirs, files in os.walk("."):
              dirs[:] = [d for d in dirs if d not in [".git", "node_modules", ".venv", "dist"]]
              for fname in files:
                  if fname.endswith(".py"):
                      fpath = os.path.join(root, fname)
                      try:
                          with open(fpath, errors="ignore") as f:
                              content = f.read()
                          if ai_import_patterns.search(content):
                              py_ai_files.append((fpath, content))
                      except:
                          pass

          print(f"[AI-Python] Found {len(py_ai_files)} AI framework files")

          for fpath, content in py_ai_files:

              # EU AI Act Art.13: Technical documentation — model must be documented
              if not os.path.exists("docs/model-card.md") and not os.path.exists("MODEL_CARD.md"):
                  violations.append({
                      "article": "EU-AI-Act-Art13", "severity": "HIGH",
                      "language": "Python/AI",
                      "file": fpath,
                      "violation": "AI model detected but no Model Card documentation found (MODEL_CARD.md)",
                      "requirement": "EU AI Act Art.13 requires technical documentation for AI systems",
                      "remediation": "Create MODEL_CARD.md documenting: model purpose, training data, performance metrics, limitations, intended use"
                  })
                  break  # only report once

              # Unsafe model loading (arbitrary code execution risk)
              if re.search(r'torch\.load\s*\([^)]*\)', content) and 'weights_only=True' not in content:
                  violations.append({
                      "article": "NIS2-Art21 / EU-AI-Act-Art15", "severity": "CRITICAL",
                      "language": "PyTorch",
                      "file": fpath,
                      "violation": f"torch.load() without weights_only=True in {os.path.basename(fpath)} — arbitrary code execution risk via malicious pickle",
                      "remediation": "Use torch.load(path, weights_only=True) — prevents RCE via malicious model files"
                  })

              # TensorFlow: loading unverified models
              if re.search(r'tf\.saved_model\.load|keras\.models\.load_model', content):
                  violations.append({
                      "article": "NIS2-Art21 / EU-AI-Act-Art15", "severity": "HIGH",
                      "language": "TensorFlow/Keras",
                      "file": fpath,
                      "violation": f"TensorFlow/Keras model loaded in {os.path.basename(fpath)} — ensure model integrity verification",
                      "remediation": "Verify model file hash (SHA-256) before loading. Store expected hash in docs/model-hashes.txt"
                  })

              # GPU memory not released
              if "torch.cuda" in content and ".cuda()" in content:
                  if "torch.cuda.empty_cache()" not in content and "del " not in content:
                      violations.append({
                          "article": "DORA-Art11", "severity": "MEDIUM",
                          "language": "PyTorch",
                          "file": fpath,
                          "violation": f"GPU tensors allocated in {os.path.basename(fpath)} without explicit memory cleanup",
                          "remediation": "Add torch.cuda.empty_cache() in finally blocks. Use context managers for GPU allocations"
                      })

              # Randomness without seed (reproducibility — required for EU AI Act Art.13)
              if re.search(r'torch\.rand|np\.random\.|random\.', content):
                  if not re.search(r'seed\s*\(|manual_seed\s*\(|set_seed\s*\(', content):
                      violations.append({
                          "article": "EU-AI-Act-Art13", "severity": "MEDIUM",
                          "language": "Python/AI",
                          "file": fpath,
                          "violation": f"Random number generation without seed in {os.path.basename(fpath)} — non-reproducible results",
                          "requirement": "EU AI Act Art.13 requires AI systems to be reproducible and auditable",
                          "remediation": "Set deterministic seeds: torch.manual_seed(42), np.random.seed(42), random.seed(42)"
                      })

              # Logging/auditability of model predictions
              if re.search(r'model\s*\(|predict\s*\(|forward\s*\(', content):
                  if not re.search(r'logging\.|logger\.|print\(.*predict|audit', content):
                      violations.append({
                          "article": "EU-AI-Act-Art13 / DORA-Art10", "severity": "MEDIUM",
                          "language": "Python/AI",
                          "file": fpath,
                          "violation": f"AI model predictions in {os.path.basename(fpath)} have no audit logging",
                          "requirement": "EU AI Act Art.13 and DORA Art.10 require logging of AI model decisions for auditability",
                          "remediation": "Add structured logging of model inputs, outputs, and confidence scores for audit trails"
                      })

          with open("${{ env.REPORT_DIR }}/gpu-ai-python-violations.json", "w") as f:
              json.dump({"ai_files_scanned": len(py_ai_files), "violations": violations}, f, indent=2)
          print(f"[AI-Python] {len(violations)} violations in {len(py_ai_files)} AI files")
          EOF

      - name: Scan Jupyter Notebooks (.ipynb)
        run: |
          python3 << 'EOF'
          import os, json, re

          try:
              import nbformat
          except ImportError:
              print("[Notebooks] nbformat not available — skipping")
              open("${{ env.REPORT_DIR }}/gpu-notebooks.json", "w").write('{"violations": []}')
              exit(0)

          violations = []
          notebook_files = []

          for root, dirs, files in os.walk("."):
              dirs[:] = [d for d in dirs if d not in [".git", "node_modules", ".venv"]]
              for fname in files:
                  if fname.endswith(".ipynb"):
                      notebook_files.append(os.path.join(root, fname))

          print(f"[Notebooks] Found {len(notebook_files)} Jupyter notebooks")

          secret_patterns = [
              r'(?i)(api[_-]?key|apikey|password|token|secret)\s*=\s*["\'][^"\']{6,}["\']',
              r'AKIA[0-9A-Z]{16}',
          ]

          for fpath in notebook_files:
              try:
                  with open(fpath) as f:
                      nb = nbformat.read(f, as_version=4)

                  for i, cell in enumerate(nb.cells):
                      if cell.cell_type != "code":
                          continue
                      source = cell.source

                      # Secrets in notebooks
                      for pat in secret_patterns:
                          if re.search(pat, source):
                              violations.append({
                                  "article": "NIS2-Art21", "severity": "CRITICAL",
                                  "language": "Jupyter Notebook",
                                  "file": f"{fpath} (cell {i+1})",
                                  "violation": "Hardcoded credential detected in notebook cell",
                                  "remediation": "Remove credentials from notebook. Use %env or dotenv. Never commit notebooks with secrets."
                              })

                      # Outputs with potential sensitive data
                      for output in cell.get("outputs", []):
                          out_text = str(output.get("text", "") or output.get("data", {}).get("text/plain", ""))
                          for pat in secret_patterns:
                              if re.search(pat, out_text):
                                  violations.append({
                                      "article": "NIS2-Art21", "severity": "CRITICAL",
                                      "language": "Jupyter Notebook",
                                      "file": f"{fpath} (cell {i+1} output)",
                                      "violation": "Potential sensitive data in notebook cell output",
                                      "remediation": "Clear all notebook outputs before committing: jupyter nbconvert --ClearOutputPreprocessor.enabled=True"
                                  })

                      # torch.load without weights_only
                      if "torch.load(" in source and "weights_only=True" not in source:
                          violations.append({
                              "article": "NIS2-Art21", "severity": "HIGH",
                              "language": "Jupyter Notebook / PyTorch",
                              "file": f"{fpath} (cell {i+1})",
                              "violation": "torch.load() without weights_only=True in notebook",
                              "remediation": "Use torch.load(path, weights_only=True)"
                          })

              except Exception as e:
                  print(f"Warning: Could not parse {fpath}: {e}")

          with open("${{ env.REPORT_DIR }}/gpu-notebooks.json", "w") as f:
              json.dump({"notebooks_scanned": len(notebook_files), "violations": violations}, f, indent=2)
          print(f"[Notebooks] {len(violations)} violations in {len(notebook_files)} notebooks")
          EOF

      - name: Scan ONNX / HuggingFace Model Files
        run: |
          python3 << 'EOF'
          import os, json, hashlib

          violations = []
          model_files = []
          model_extensions = [".onnx", ".pt", ".pth", ".h5", ".pb", ".bin", ".safetensors", ".gguf"]

          for root, dirs, files in os.walk("."):
              dirs[:] = [d for d in dirs if d not in [".git", "node_modules"]]
              for fname in files:
                  if any(fname.endswith(ext) for ext in model_extensions):
                      model_files.append(os.path.join(root, fname))

          print(f"[Models] Found {len(model_files)} model files")

          # Check for model hash verification file
          hash_files = ["docs/model-hashes.txt", "model-hashes.sha256", "checksums.txt"]
          hash_file_found = any(os.path.exists(p) for p in hash_files)

          for fpath in model_files:
              size_mb = os.path.getsize(fpath) / (1024 * 1024)

              if not hash_file_found:
                  violations.append({
                      "article": "EU-AI-Act-Art13 / NIS2-Art21", "severity": "HIGH",
                      "file": fpath,
                      "violation": f"Model file {os.path.basename(fpath)} ({size_mb:.1f}MB) has no integrity verification (hash file missing)",
                      "requirement": "EU AI Act Art.13 requires model integrity verification. NIS2 Art.21 requires supply chain security.",
                      "remediation": "Create docs/model-hashes.txt with SHA-256 hash of each model file. Verify on load."
                  })

              # Check: model card exists for this model
              model_name = os.path.splitext(os.path.basename(fpath))[0]
              if not os.path.exists("MODEL_CARD.md") and not os.path.exists(f"docs/{model_name}-card.md"):
                  violations.append({
                      "article": "EU-AI-Act-Art13", "severity": "HIGH",
                      "file": fpath,
                      "violation": f"No Model Card found for {os.path.basename(fpath)}",
                      "requirement": "EU AI Act Art.13 requires technical documentation for deployed AI models",
                      "remediation": "Create MODEL_CARD.md documenting model purpose, training data, performance benchmarks, limitations"
                  })

          with open("${{ env.REPORT_DIR }}/gpu-model-files.json", "w") as f:
              json.dump({"model_files_found": len(model_files), "violations": violations}, f, indent=2)
          print(f"[Models] {len(violations)} violations for {len(model_files)} model files")
          EOF

      - name: GPU Resource & Configuration Compliance
        run: |
          python3 << 'EOF'
          import os, json, re

          violations = []

          # Check Kubernetes GPU resource limits
          k8s_files = []
          for root, dirs, files in os.walk("."):
              dirs[:] = [d for d in dirs if d not in [".git", "node_modules"]]
              for fname in files:
                  if fname.endswith((".yaml", ".yml")) and any(k in root.lower() for k in ["k8s", "kubernetes", "helm", "charts", "deploy", "manifest"]):
                      k8s_files.append(os.path.join(root, fname))

          for fpath in k8s_files:
              try:
                  with open(fpath) as f:
                      content = f.read()

                  # GPU allocated without resource limits
                  if "nvidia.com/gpu" in content:
                      if "limits:" not in content:
                          violations.append({
                              "article": "DORA-Art8 / NIS2-Art21", "severity": "HIGH",
                              "file": fpath,
                              "violation": "GPU resource requested in Kubernetes manifest without resource limits",
                              "remediation": "Add limits: nvidia.com/gpu to prevent resource exhaustion"
                          })

                      # Check for GPU workloads without security context
                      if "securityContext" not in content:
                          violations.append({
                              "article": "NIS2-Art21", "severity": "HIGH",
                              "file": fpath,
                              "violation": "GPU workload in Kubernetes manifest without securityContext",
                              "remediation": "Add securityContext: runAsNonRoot: true, readOnlyRootFilesystem: true"
                          })

                      # Check for privileged containers (common GPU anti-pattern)
                      if re.search(r'privileged\s*:\s*true', content):
                          violations.append({
                              "article": "NIS2-Art21", "severity": "CRITICAL",
                              "file": fpath,
                              "violation": "GPU container running as privileged — full host kernel access",
                              "remediation": "Remove privileged: true. Use device plugins instead: nvidia.com/gpu resource requests"
                          })

              except:
                  pass

          # Check Docker/container GPU configuration
          dockerfiles = [f for root, dirs, files in os.walk(".") for f in files
                        if f in ["Dockerfile", "Dockerfile.gpu", "Dockerfile.cuda"]]

          for dockerfile in dockerfiles:
              try:
                  with open(dockerfile) as f:
                      content = f.read()

                  if "cuda" in content.lower() or "nvidia" in content.lower():
                      # Check for non-root user in GPU containers
                      if "USER root" in content or "USER 0" in content:
                          violations.append({
                              "article": "NIS2-Art21", "severity": "HIGH",
                              "file": dockerfile,
                              "violation": "GPU/CUDA container running as root",
                              "remediation": "Add USER directive with non-root user. Create dedicated user: RUN useradd -m appuser && USER appuser"
                          })
              except:
                  pass

          with open("${{ env.REPORT_DIR }}/gpu-resource-compliance.json", "w") as f:
              json.dump(violations, f, indent=2)
          print(f"[GPU-Resources] {len(violations)} violations")
          EOF

      - uses: actions/upload-artifact@v4
        with:
          name: gpu-ai-reports
          path: ${{ env.REPORT_DIR }}/gpu-*.json


  # ============================================================
  # JOB 6: Consolidated Report
  # ============================================================
  generate-report:
    name: "Generate Consolidated EU Compliance Report"
    runs-on: ubuntu-latest
    needs:
      - sbom-generation
      - dora-chapter2-risk-management
      - dora-chapter3-incident-reporting
      - dora-chapter4-resilience-testing
      - dora-chapter5-third-party
      - nis2-compliance
      - multilang-code-scan
      - gpu-ai-compliance
    if: always()

    steps:
      - uses: actions/checkout@v4

      - name: Download all reports
        uses: actions/download-artifact@v4
        with:
          path: all-reports/

      - name: Consolidate all violations
        run: |
          python3 << 'EOF'
          import json, os, glob
          from datetime import datetime

          categories = {
              "dora_ch2_risk":        [],
              "dora_ch3_incident":    [],
              "dora_ch4_resilience":  [],
              "dora_ch5_third_party": [],
              "nis2":                 [],
              "sbom":                 [],
              "multilang":            [],
              "gpu_ai":               [],
          }

          category_map = {
              "dora-art5": "dora_ch2_risk",  "dora-art8": "dora_ch2_risk",
              "dora-art9": "dora_ch2_risk",  "dora-art10": "dora_ch2_risk",
              "dora-art11": "dora_ch2_risk", "dora-art12": "dora_ch2_risk",
              "dora-art13": "dora_ch2_risk",
              "dora-art17": "dora_ch3_incident", "dora-art18": "dora_ch3_incident",
              "dora-art19": "dora_ch3_incident",
              "dora-art24": "dora_ch4_resilience", "dora-art25": "dora_ch4_resilience",
              "dora-art26": "dora_ch4_resilience",
              "dora-art28": "dora_ch5_third_party", "dora-art29": "dora_ch5_third_party",
              "dora-art30": "dora_ch5_third_party", "dora-art44": "dora_ch5_third_party",
              "nis2":       "nis2",
              "sbom":       "sbom",
              "lang-":      "multilang",
              "gpu-":       "gpu_ai",
          }

          for fpath in glob.glob("all-reports/**/*.json", recursive=True):
              fname = os.path.basename(fpath).lower()
              try:
                  with open(fpath) as f:
                      data = json.load(f)

                  items = []
                  if isinstance(data, list):
                      items = data
                  elif isinstance(data, dict):
                      items = data.get("violations", [])

                  target_cat = None
                  for key, cat in category_map.items():
                      if fname.startswith(key):
                          target_cat = cat
                          break

                  if target_cat and items:
                      categories[target_cat].extend(items)

              except Exception as e:
                  print(f"Warning: {fpath}: {e}")

          # Summary
          severity_count = {"CRITICAL": 0, "HIGH": 0, "MEDIUM": 0, "LOW": 0, "INFO": 0}
          all_items = [item for cat in categories.values() for item in cat]
          for item in all_items:
              sev = item.get("severity", "INFO").upper()
              if sev in severity_count:
                  severity_count[sev] += 1

          report = {
              "scan_date": datetime.utcnow().isoformat() + "Z",
              "repository": os.environ.get("GITHUB_REPOSITORY", "unknown"),
              "commit": os.environ.get("GITHUB_SHA", "unknown")[:8],
              "scanner_version": "2.0",
              "regulations_checked": ["DORA (EU) 2022/2554 - Ch.II-V", "NIS2 (EU) 2022/2555 - Art.21,23", "SBOM CycloneDX 1.6 + SPDX 2.3", "EU AI Act - Art.13,15"],
              "languages_scanned": ["Python", "JavaScript", "TypeScript", "Go", "Java", "Rust", "C", "C++", "Ruby", "PHP", "CUDA"],
              "summary": {**severity_count, "total": len(all_items)},
              **categories
          }

          os.makedirs("final-report", exist_ok=True)
          with open("final-report/compliance-violations-v2.json", "w") as f:
              json.dump(report, f, indent=2)

          print("\n" + "="*60)
          print("EU COMPLIANCE SCANNER v2.0 — FULL REPORT SUMMARY")
          print("="*60)
          print(f"Repo      : {report['repository']} @ {report['commit']}")
          print(f"Date      : {report['scan_date']}")
          print("="*60)
          print(f"DORA Ch.II  (Risk Mgmt)      : {len(categories['dora_ch2_risk'])} violations")
          print(f"DORA Ch.III (Incident)        : {len(categories['dora_ch3_incident'])} violations")
          print(f"DORA Ch.IV  (Resilience)      : {len(categories['dora_ch4_resilience'])} violations")
          print(f"DORA Ch.V   (Third-Party)     : {len(categories['dora_ch5_third_party'])} violations")
          print(f"NIS2                          : {len(categories['nis2'])} violations")
          print(f"SBOM                          : {len(categories['sbom'])} violations")
          print(f"Multi-Language Code           : {len(categories['multilang'])} violations")
          print(f"GPU / AI Code                 : {len(categories['gpu_ai'])} violations")
          print("="*60)
          print(f"TOTAL     : {len(all_items)}")
          print(f"  🔴 CRITICAL : {severity_count['CRITICAL']}")
          print(f"  🟠 HIGH     : {severity_count['HIGH']}")
          print(f"  🟡 MEDIUM   : {severity_count['MEDIUM']}")
          print(f"  🔵 LOW/INFO : {severity_count['LOW'] + severity_count['INFO']}")
          print("="*60)
          EOF

      - name: Generate Markdown Report
        run: |
          python3 << 'EOF'
          import json

          with open("final-report/compliance-violations-v2.json") as f:
              d = json.load(f)

          s = d["summary"]
          sev_icon = {"CRITICAL": "🔴", "HIGH": "🟠", "MEDIUM": "🟡", "LOW": "🔵", "INFO": "⚪"}

          md = f"""# 🇪🇺 EU Compliance Scanner v2.0 — Violation Report

          **Repo:** {d['repository']}  |  **Commit:** `{d['commit']}`  |  **Date:** {d['scan_date']}

          ## Executive Summary

          | Category | Violations |
          |---|---|
          | DORA Ch.II — ICT Risk Management (Art.5–16) | {len(d['dora_ch2_risk'])} |
          | DORA Ch.III — Incident Reporting (Art.17–23) | {len(d['dora_ch3_incident'])} |
          | DORA Ch.IV — Resilience Testing (Art.24–27) | {len(d['dora_ch4_resilience'])} |
          | DORA Ch.V — Third-Party Risk (Art.28–44) | {len(d['dora_ch5_third_party'])} |
          | NIS2 Art.21 + Art.23 | {len(d['nis2'])} |
          | SBOM (CycloneDX + SPDX) | {len(d['sbom'])} |
          | Multi-Language Code Scan | {len(d['multilang'])} |
          | GPU / AI Code Compliance | {len(d['gpu_ai'])} |
          | **TOTAL** | **{s['total']}** |

          | Severity | Count |
          |---|---|
          | 🔴 CRITICAL | {s['CRITICAL']} |
          | 🟠 HIGH | {s['HIGH']} |
          | 🟡 MEDIUM | {s['MEDIUM']} |
          | 🔵 LOW/INFO | {s.get('LOW', 0) + s.get('INFO', 0)} |

          > Languages scanned: {', '.join(d.get('languages_scanned', []))}

          ---
          """

          def render_violations(items, title):
              if not items:
                  return f"\n## {title}\n\n✅ No violations found.\n"
              out = f"\n## {title}\n\n"
              for v in items:
                  sev = v.get("severity", "INFO")
                  icon = sev_icon.get(sev, "⚪")
                  art = v.get("article", "")
                  msg = v.get("violation", "")
                  rem = v.get("remediation", "")
                  lang = v.get("language", "")
                  lang_str = f" `[{lang}]`" if lang else ""
                  out += f"### {icon} `{art}`{lang_str} — {msg}\n"
                  out += f"**Severity:** {sev}  \n"
                  if rem:
                      out += f"**Fix:** {rem}  \n"
                  out += "\n"
              return out

          md += render_violations(d['dora_ch2_risk'],       "DORA Chapter II — ICT Risk Management (Art.5–16)")
          md += render_violations(d['dora_ch3_incident'],   "DORA Chapter III — Incident Management (Art.17–23)")
          md += render_violations(d['dora_ch4_resilience'], "DORA Chapter IV — Resilience Testing (Art.24–27)")
          md += render_violations(d['dora_ch5_third_party'],"DORA Chapter V — Third-Party Risk (Art.28–44)")
          md += render_violations(d['nis2'],                "NIS2 Directive (Art.21 + Art.23)")
          md += render_violations(d['sbom'],                "SBOM Compliance (CycloneDX + SPDX)")
          md += render_violations(d['multilang'],           "Multi-Language Code Security")
          md += render_violations(d['gpu_ai'],              "GPU / AI Code Compliance (CUDA, PyTorch, TF, ONNX, Notebooks)")

          with open("final-report/compliance-report-v2.md", "w") as f:
              f.write(md)
          print("Markdown report generated")
          EOF

      - uses: actions/upload-artifact@v4
        with:
          name: eu-compliance-v2-full-report
          path: final-report/
          retention-days: 365

      - name: Post PR comment
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('final-report/compliance-report-v2.md', 'utf8');
            const preview = report.substring(0, 4000) + '\n\n_Full report in Actions artifacts._';
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `## 🇪🇺 EU Compliance Scanner v2.0\n\n${preview}`
            });

      - name: Enforce compliance gate
        if: env.FAIL_ON_CRITICAL == 'true'
        run: |
          CRITICAL=$(python3 -c "import json; d=json.load(open('final-report/compliance-violations-v2.json')); print(d['summary']['CRITICAL'])")
          echo "Critical violations: $CRITICAL"
          if [ "$CRITICAL" -gt "0" ]; then
            echo "❌ PIPELINE FAILED: $CRITICAL CRITICAL violations found"
            exit 1
          fi
          echo "✅ No critical violations — pipeline passed"
